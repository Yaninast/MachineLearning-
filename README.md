# NLP-projects
The following steps have been performed: (1) perform data vectorization; (2) determine the characteristics that define the context or extract the most representative words of the documents; (3) apply k-means algorithm; (4) apply labels and identify issues for clustering; (5) use Word2Vec matrix to determine words which are similar and create equivalent classes; (5) build corpus ontology (6) validation/confirmation of clusters. During the analysis, 7 -means clustering has been performed, and the quality of the resulted clusters have been assessed by comparing them to the manually constructed clusters. After 7- means algorithm has been executed the certain steps have been taken to improve the quality of the clusters such as creating equivalence classes to force the documents towards the desired clusters. Domain knowledge in the form of the ontology has been used to manipulate, label and evaluate the clusters. Finally, when the growing number of ECs started to deteriorate the clustering performance and it seemed that the results could not be improved any further, the need for more sophisticated data representation has been identified. BERT(Bidirectional Embedding Representations from Transformers) model has been fine- tuned to apply to the given corpus of the documents and obtain contextual document representation that would capture the connections between the documents more precisely by understanding their contexts. The next step of the analysis is to increase the training data and to use the bidirectional embeddings to improve clustering results.
